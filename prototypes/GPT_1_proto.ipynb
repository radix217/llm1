{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba67f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f1f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm deliberately not using the inbuilt torch modules like attention, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58612f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ae71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dropout=0.1):\n",
    "        super().__init()\n",
    "        assert dim_model % num_heads == 0, \"dim_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_k = dim_model // num_heads # we are assuming dim_k = dim_v\n",
    "\n",
    "        self.query_projections = nn.Linear(dim_model, self.dim_k * num_heads)\n",
    "        self.key_projections = nn.Linear(dim_model, self.dim_k * num_heads)\n",
    "        self.value_projections = nn.Linear(dim_model, self.dim_k * num_heads)\n",
    "        self.fc = nn.Linear(self.dim_k * num_heads, dim_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        batch_size, seq_length = x.shape[:2]\n",
    "\n",
    "        # Shape: (batch_size, num_heads, seq_length, dim_k)\n",
    "        queries = self.query_projections(x).view(batch_size, seq_length, self.num_heads, self.dim_k).transpose(1,2)\n",
    "        keys = self.key_projections(x).view(batch_size, seq_length, self.num_heads, self.dim_k).transpose(1,2)\n",
    "        values = self.value_projections(x).view(batch_size, seq_length, self.num_heads, self.dim_k).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(queries, keys.transpose(-1,-2)) / (self.dim_k ** 0.5)\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        multi_context = torch.matmul(attention_weights, values)\n",
    "        context = self.fc(multi_context.transpose(1,2).contiguous().view(batch_size, seq_length, self.num_heads * self.dim_k))\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multi_attention = MultiHeadAttention(num_heads, dim_model)\n",
    "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=dim_model)\n",
    "        self.attention_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        #FFN\n",
    "        self.fc1 = nn.Linear(dim_model, dim_model * 4)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.fc2 = nn.Linear(dim_model * 4, dim_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(normalized_shape=dim_model)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)  # GPT-1 style\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape[:2]\n",
    "\n",
    "        mask = torch.tril(torch.ones(seq_length, seq_length, device=x.device)).bool()\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        context = self.masked_multi_attention(x, mask=mask)\n",
    "        context = self.attention_dropout(context)\n",
    "\n",
    "        x = context + x # residual connection\n",
    "        layer_norm_1 = self.layer_norm_1(x)\n",
    "\n",
    "        #FFN\n",
    "        f1 = self.fc1(layer_norm_1)\n",
    "        act1 = self.gelu1(f1)\n",
    "        f2 = self.fc2(act1)\n",
    "        ffn_dropout = self.ffn_dropout(f2)\n",
    "\n",
    "        f_out = ffn_dropout + layer_norm_1 # residual connection\n",
    "        layer_norm_2 = self.layer_norm_2(f_out) # Post-LN as in GPT-1\n",
    "        return layer_norm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e979b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT1(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_model, num_heads, num_decoder_layers, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.positional_embedding = nn.Embedding(max_seq_length, dim_model)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(dim_model, num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)  # final LN\n",
    "        self.output_head = nn.Linear(dim_model, vocab_size, bias=False)\n",
    "        self.output_head.weight = self.token_embedding.weight\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        positions = torch.arange(0, seq_length, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        x = self.token_embedding(input_ids) + self.positional_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_head(x)\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_targets = targets[..., 1:].contiguous()\n",
    "        loss = nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
    "        return logits, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
