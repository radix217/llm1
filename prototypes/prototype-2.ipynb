{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ba67f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56f1f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm deliberately not using the inbuilt torch modules like attention, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bb1047f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim_in, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.fc = nn.Linear(dim_in, intermediate_size * 2, bias=False)\n",
    "        self.swish = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x) # [..., 2 * intermediate_size]\n",
    "        gate, val = out.chunk(2, dim=-1) # each [..., intermediate_size]\n",
    "        return self.swish(gate) * val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9f4c911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self):\n",
    "        self.cos_cache = None\n",
    "        self.sin_cache = None\n",
    "        self.theta = None\n",
    "        self.cached_seq = 0\n",
    "        self.cached_d = 0\n",
    "\n",
    "    def get_rot_cached(self, d, seq_length):\n",
    "        if self.cos_cache is None:\n",
    "            self.theta = 1e6 ** (-2 * torch.arange(d//2, dtype=torch.float32) / d)\n",
    "            ms = torch.arange(seq_length)\n",
    "            angles = torch.einsum('i,j->ij', ms, self.theta)\n",
    "            self.cos_cache = torch.cos(angles)\n",
    "            self.sin_cache = torch.sin(angles)\n",
    "            self.cached_d = d\n",
    "            self.cached_seq = seq_length\n",
    "            needed_dhalf = d // 2\n",
    "            return self.cos_cache[:seq_length, :needed_dhalf], self.sin_cache[:seq_length, :needed_dhalf]\n",
    "\n",
    "        needed_dhalf = d // 2\n",
    "\n",
    "        # Recompute if dimension changes\n",
    "        if d != self.cached_d:\n",
    "            self.theta = 1e6 ** (-2 * torch.arange(needed_dhalf, dtype=torch.float32) / d)\n",
    "            ms = torch.arange(self.cached_seq)\n",
    "            angles = torch.einsum('i,j->ij', ms, self.theta)\n",
    "            self.cos_cache = torch.cos(angles)\n",
    "            self.sin_cache = torch.sin(angles)\n",
    "            self.cached_d = d\n",
    "\n",
    "        # Extend sequence length if needed\n",
    "        if seq_length > self.cached_seq:\n",
    "            new_ms = torch.arange(self.cached_seq, seq_length, dtype=torch.float32)\n",
    "            new_angles = torch.einsum('i,j->ij', new_ms, self.theta)\n",
    "            new_cos = torch.cos(new_angles)\n",
    "            new_sin = torch.sin(new_angles)\n",
    "            self.cos_cache = torch.cat([self.cos_cache, new_cos], dim=0)\n",
    "            self.sin_cache = torch.cat([self.sin_cache, new_sin], dim=0)\n",
    "            self.cached_seq = seq_length\n",
    "\n",
    "        # Return the sliced views as needed\n",
    "        return self.cos_cache[:seq_length, :needed_dhalf], self.sin_cache[:seq_length, :needed_dhalf]\n",
    "\n",
    "    def apply(self, t):\n",
    "        # (batch_size, heads, seq_length, d)\n",
    "        seq_length, d = t.shape[-2:]\n",
    "\n",
    "        r_cos, r_sin = self.get_rot_cached(d, seq_length)\n",
    "\n",
    "        t_even = t[..., 0::2]\n",
    "        t_odd = t[..., 1::2]\n",
    "        t_conj = torch.empty_like(t)\n",
    "        t_conj[..., 0::2] = -t_odd\n",
    "        t_conj[..., 1::2] = t_even\n",
    "\n",
    "        return t * r_cos.repeat_interleave(2, dim=-1) + t_conj * r_sin.repeat_interleave(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c53ae71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, num_q_heads, group_size, dim_model, dim_k, dropout=0.1, enable_rope = True):\n",
    "        super().__init__()\n",
    "        assert dim_model % num_q_heads == 0, \"dim_model must be divisible by num_q_heads\"\n",
    "        assert num_q_heads % group_size == 0, \"num_q_heads must be divisible by group_size\"\n",
    "\n",
    "        self.enable_rope = enable_rope\n",
    "        self.group_size = group_size\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_q_heads // group_size\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_k = dim_k # we are assuming dim_k = dim_v\n",
    "\n",
    "        self.rope = None\n",
    "        if self.enable_rope:\n",
    "            self.rope = RoPE()\n",
    "\n",
    "        self.q_proj = nn.Linear(dim_model, self.dim_k * self.num_q_heads, bias=False)\n",
    "        self.k_proj = nn.Linear(dim_model, self.dim_k * self.num_kv_heads, bias=False)\n",
    "        self.v_proj = nn.Linear(dim_model, self.dim_k * self.num_kv_heads, bias=False)\n",
    "        self.fc = nn.Linear(self.dim_k * num_q_heads, dim_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        batch_size, seq_length = x.shape[:2]\n",
    "\n",
    "        q = self.q_proj(x).view(batch_size, seq_length, self.num_q_heads, self.dim_k).transpose(1,2) # (batch_size, num_q_heads, seq_length, dim_k)\n",
    "        k = self.k_proj(x).view(batch_size, seq_length, self.num_kv_heads, self.dim_k).transpose(1,2) # (batch_size, num_kv_heads, seq_length, dim_k)\n",
    "        v = self.v_proj(x).view(batch_size, seq_length, self.num_kv_heads, self.dim_k).transpose(1,2) # (batch_size, num_kv_heads, seq_length, dim_k)\n",
    "\n",
    "        if self.enable_rope:\n",
    "            q = self.rope.apply(q)\n",
    "            k = self.rope.apply(k)\n",
    "\n",
    "        q = q.view(batch_size, self.num_kv_heads, self.group_size, seq_length, self.dim_k)\n",
    "\n",
    "        k = k.transpose(-1,-2) # (batch_size, num_kv_heads, dim_k, seq_length)\n",
    "        k = k.unsqueeze(2) # (batch_size, num_kv_heads, 1, dim_k, seq_length)\n",
    "\n",
    "        scores = torch.matmul(q,k) / math.sqrt(self.dim_k) # (batch_size, num_kv_heads, group_size, seq_length, seq_length)\n",
    "\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        v = v.unsqueeze(2) # (batch_size, num_kv_heads, 1, seq_length, dim_k)\n",
    "        out = torch.matmul(attn_weights, v) # (batch_size, num_kv_heads, group_size, seq_length, dim_k)\n",
    "\n",
    "        out = out.contiguous().view(batch_size, self.num_q_heads, seq_length, self.dim_k)\n",
    "        out = self.fc(out.transpose(1,2).contiguous().view(batch_size, seq_length, self.num_q_heads * self.dim_k))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ee0e7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_model, dim_k, num_q_heads, group_size, Intermediate_size, eps=1e-6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.gq_attn = GroupedQueryAttention(\n",
    "            num_q_heads,\n",
    "            group_size,\n",
    "            dim_model,\n",
    "            dim_k,\n",
    "            dropout=dropout,\n",
    "            enable_rope = True)\n",
    "        self.rms_norm_1 = nn.RMSNorm(normalized_shape=dim_model, eps=eps)\n",
    "        self.attention_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        #FFN\n",
    "        self.swiglu = SwiGLU(dim_in=dim_model, intermediate_size=Intermediate_size)\n",
    "        self.down_proj = nn.Linear(Intermediate_size, dim_model, bias=False)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.rms_norm_2 = nn.RMSNorm(normalized_shape=dim_model, eps=eps)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.RMSNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape[:2]\n",
    "\n",
    "        mask = torch.tril(torch.ones(seq_length, seq_length, device=x.device)).bool()\n",
    "        mask = mask[None, None, None, :, :]\n",
    "\n",
    "        norm1 = self.rms_norm_1(x)\n",
    "        context = self.gq_attn(norm1, mask=mask)\n",
    "        context = self.attention_dropout(context)\n",
    "        x = context + x\n",
    "\n",
    "        norm2 = self.rms_norm_2(x)\n",
    "        act = self.swiglu(norm2)\n",
    "        ffn_out = self.down_proj(act)\n",
    "        ffn_dropout = self.ffn_dropout(ffn_out)\n",
    "        x = ffn_dropout + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0e979b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proto2(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_model, dim_k, num_q_heads, group_size, num_decoder_layers, Intermediate_size, eps=1e-6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim_model)\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(\n",
    "                dim_model,\n",
    "                dim_k,\n",
    "                num_q_heads,\n",
    "                group_size,\n",
    "                Intermediate_size,\n",
    "                eps,\n",
    "                dropout=dropout\n",
    "                ) for _ in range(num_decoder_layers)]\n",
    "            )\n",
    "        self.rms_norm = nn.RMSNorm(dim_model, eps)  # final LN\n",
    "        self.output_head = nn.Linear(dim_model, vocab_size, bias=False)\n",
    "        self.output_head.weight = self.token_embedding.weight\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif isinstance(module, nn.RMSNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        x = self.rms_norm(x)\n",
    "        logits = self.output_head(x)\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_targets = targets[..., 1:].contiguous()\n",
    "        loss = nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ada7be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 151936 # random for demo\n",
    "\n",
    "model = Proto2(\n",
    "    vocab_size=vocab_size,\n",
    "    dim_model=1024,\n",
    "    dim_k=1024//8, # possibly derived from dim_model // (num_q_heads // group_size)\n",
    "    num_q_heads=16,\n",
    "    group_size=2,\n",
    "    num_decoder_layers=28,\n",
    "    Intermediate_size=3072,\n",
    "    eps=1e-6,\n",
    "    dropout=0.1 # should drop to 0.0 in finetuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a1072a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: token_embedding.weight\n",
      "Parameter shape: torch.Size([151936, 1024])\n",
      "\n",
      "Total params: 155582464\n",
      "\n",
      "Parameter name: decoder_layers.0.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 157679616\n",
      "\n",
      "Parameter name: decoder_layers.0.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 158728192\n",
      "\n",
      "Parameter name: decoder_layers.0.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 159776768\n",
      "\n",
      "Parameter name: decoder_layers.0.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 161873920\n",
      "\n",
      "Parameter name: decoder_layers.0.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 161874944\n",
      "\n",
      "Parameter name: decoder_layers.0.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 168166400\n",
      "\n",
      "Parameter name: decoder_layers.0.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 171312128\n",
      "\n",
      "Parameter name: decoder_layers.0.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 171313152\n",
      "\n",
      "Parameter name: decoder_layers.1.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 173410304\n",
      "\n",
      "Parameter name: decoder_layers.1.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 174458880\n",
      "\n",
      "Parameter name: decoder_layers.1.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 175507456\n",
      "\n",
      "Parameter name: decoder_layers.1.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 177604608\n",
      "\n",
      "Parameter name: decoder_layers.1.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 177605632\n",
      "\n",
      "Parameter name: decoder_layers.1.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 183897088\n",
      "\n",
      "Parameter name: decoder_layers.1.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 187042816\n",
      "\n",
      "Parameter name: decoder_layers.1.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 187043840\n",
      "\n",
      "Parameter name: decoder_layers.2.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 189140992\n",
      "\n",
      "Parameter name: decoder_layers.2.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 190189568\n",
      "\n",
      "Parameter name: decoder_layers.2.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 191238144\n",
      "\n",
      "Parameter name: decoder_layers.2.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 193335296\n",
      "\n",
      "Parameter name: decoder_layers.2.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 193336320\n",
      "\n",
      "Parameter name: decoder_layers.2.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 199627776\n",
      "\n",
      "Parameter name: decoder_layers.2.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 202773504\n",
      "\n",
      "Parameter name: decoder_layers.2.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 202774528\n",
      "\n",
      "Parameter name: decoder_layers.3.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 204871680\n",
      "\n",
      "Parameter name: decoder_layers.3.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 205920256\n",
      "\n",
      "Parameter name: decoder_layers.3.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 206968832\n",
      "\n",
      "Parameter name: decoder_layers.3.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 209065984\n",
      "\n",
      "Parameter name: decoder_layers.3.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 209067008\n",
      "\n",
      "Parameter name: decoder_layers.3.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 215358464\n",
      "\n",
      "Parameter name: decoder_layers.3.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 218504192\n",
      "\n",
      "Parameter name: decoder_layers.3.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 218505216\n",
      "\n",
      "Parameter name: decoder_layers.4.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 220602368\n",
      "\n",
      "Parameter name: decoder_layers.4.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 221650944\n",
      "\n",
      "Parameter name: decoder_layers.4.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 222699520\n",
      "\n",
      "Parameter name: decoder_layers.4.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 224796672\n",
      "\n",
      "Parameter name: decoder_layers.4.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 224797696\n",
      "\n",
      "Parameter name: decoder_layers.4.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 231089152\n",
      "\n",
      "Parameter name: decoder_layers.4.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 234234880\n",
      "\n",
      "Parameter name: decoder_layers.4.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 234235904\n",
      "\n",
      "Parameter name: decoder_layers.5.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 236333056\n",
      "\n",
      "Parameter name: decoder_layers.5.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 237381632\n",
      "\n",
      "Parameter name: decoder_layers.5.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 238430208\n",
      "\n",
      "Parameter name: decoder_layers.5.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 240527360\n",
      "\n",
      "Parameter name: decoder_layers.5.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 240528384\n",
      "\n",
      "Parameter name: decoder_layers.5.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 246819840\n",
      "\n",
      "Parameter name: decoder_layers.5.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 249965568\n",
      "\n",
      "Parameter name: decoder_layers.5.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 249966592\n",
      "\n",
      "Parameter name: decoder_layers.6.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 252063744\n",
      "\n",
      "Parameter name: decoder_layers.6.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 253112320\n",
      "\n",
      "Parameter name: decoder_layers.6.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 254160896\n",
      "\n",
      "Parameter name: decoder_layers.6.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 256258048\n",
      "\n",
      "Parameter name: decoder_layers.6.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 256259072\n",
      "\n",
      "Parameter name: decoder_layers.6.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 262550528\n",
      "\n",
      "Parameter name: decoder_layers.6.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 265696256\n",
      "\n",
      "Parameter name: decoder_layers.6.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 265697280\n",
      "\n",
      "Parameter name: decoder_layers.7.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 267794432\n",
      "\n",
      "Parameter name: decoder_layers.7.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 268843008\n",
      "\n",
      "Parameter name: decoder_layers.7.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 269891584\n",
      "\n",
      "Parameter name: decoder_layers.7.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 271988736\n",
      "\n",
      "Parameter name: decoder_layers.7.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 271989760\n",
      "\n",
      "Parameter name: decoder_layers.7.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 278281216\n",
      "\n",
      "Parameter name: decoder_layers.7.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 281426944\n",
      "\n",
      "Parameter name: decoder_layers.7.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 281427968\n",
      "\n",
      "Parameter name: decoder_layers.8.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 283525120\n",
      "\n",
      "Parameter name: decoder_layers.8.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 284573696\n",
      "\n",
      "Parameter name: decoder_layers.8.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 285622272\n",
      "\n",
      "Parameter name: decoder_layers.8.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 287719424\n",
      "\n",
      "Parameter name: decoder_layers.8.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 287720448\n",
      "\n",
      "Parameter name: decoder_layers.8.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 294011904\n",
      "\n",
      "Parameter name: decoder_layers.8.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 297157632\n",
      "\n",
      "Parameter name: decoder_layers.8.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 297158656\n",
      "\n",
      "Parameter name: decoder_layers.9.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 299255808\n",
      "\n",
      "Parameter name: decoder_layers.9.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 300304384\n",
      "\n",
      "Parameter name: decoder_layers.9.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 301352960\n",
      "\n",
      "Parameter name: decoder_layers.9.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 303450112\n",
      "\n",
      "Parameter name: decoder_layers.9.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 303451136\n",
      "\n",
      "Parameter name: decoder_layers.9.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 309742592\n",
      "\n",
      "Parameter name: decoder_layers.9.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 312888320\n",
      "\n",
      "Parameter name: decoder_layers.9.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 312889344\n",
      "\n",
      "Parameter name: decoder_layers.10.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 314986496\n",
      "\n",
      "Parameter name: decoder_layers.10.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 316035072\n",
      "\n",
      "Parameter name: decoder_layers.10.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 317083648\n",
      "\n",
      "Parameter name: decoder_layers.10.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 319180800\n",
      "\n",
      "Parameter name: decoder_layers.10.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 319181824\n",
      "\n",
      "Parameter name: decoder_layers.10.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 325473280\n",
      "\n",
      "Parameter name: decoder_layers.10.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 328619008\n",
      "\n",
      "Parameter name: decoder_layers.10.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 328620032\n",
      "\n",
      "Parameter name: decoder_layers.11.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 330717184\n",
      "\n",
      "Parameter name: decoder_layers.11.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 331765760\n",
      "\n",
      "Parameter name: decoder_layers.11.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 332814336\n",
      "\n",
      "Parameter name: decoder_layers.11.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 334911488\n",
      "\n",
      "Parameter name: decoder_layers.11.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 334912512\n",
      "\n",
      "Parameter name: decoder_layers.11.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 341203968\n",
      "\n",
      "Parameter name: decoder_layers.11.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 344349696\n",
      "\n",
      "Parameter name: decoder_layers.11.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 344350720\n",
      "\n",
      "Parameter name: decoder_layers.12.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 346447872\n",
      "\n",
      "Parameter name: decoder_layers.12.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 347496448\n",
      "\n",
      "Parameter name: decoder_layers.12.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 348545024\n",
      "\n",
      "Parameter name: decoder_layers.12.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 350642176\n",
      "\n",
      "Parameter name: decoder_layers.12.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 350643200\n",
      "\n",
      "Parameter name: decoder_layers.12.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 356934656\n",
      "\n",
      "Parameter name: decoder_layers.12.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 360080384\n",
      "\n",
      "Parameter name: decoder_layers.12.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 360081408\n",
      "\n",
      "Parameter name: decoder_layers.13.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 362178560\n",
      "\n",
      "Parameter name: decoder_layers.13.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 363227136\n",
      "\n",
      "Parameter name: decoder_layers.13.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 364275712\n",
      "\n",
      "Parameter name: decoder_layers.13.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 366372864\n",
      "\n",
      "Parameter name: decoder_layers.13.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 366373888\n",
      "\n",
      "Parameter name: decoder_layers.13.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 372665344\n",
      "\n",
      "Parameter name: decoder_layers.13.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 375811072\n",
      "\n",
      "Parameter name: decoder_layers.13.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 375812096\n",
      "\n",
      "Parameter name: decoder_layers.14.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 377909248\n",
      "\n",
      "Parameter name: decoder_layers.14.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 378957824\n",
      "\n",
      "Parameter name: decoder_layers.14.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 380006400\n",
      "\n",
      "Parameter name: decoder_layers.14.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 382103552\n",
      "\n",
      "Parameter name: decoder_layers.14.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 382104576\n",
      "\n",
      "Parameter name: decoder_layers.14.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 388396032\n",
      "\n",
      "Parameter name: decoder_layers.14.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 391541760\n",
      "\n",
      "Parameter name: decoder_layers.14.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 391542784\n",
      "\n",
      "Parameter name: decoder_layers.15.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 393639936\n",
      "\n",
      "Parameter name: decoder_layers.15.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 394688512\n",
      "\n",
      "Parameter name: decoder_layers.15.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 395737088\n",
      "\n",
      "Parameter name: decoder_layers.15.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 397834240\n",
      "\n",
      "Parameter name: decoder_layers.15.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 397835264\n",
      "\n",
      "Parameter name: decoder_layers.15.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 404126720\n",
      "\n",
      "Parameter name: decoder_layers.15.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 407272448\n",
      "\n",
      "Parameter name: decoder_layers.15.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 407273472\n",
      "\n",
      "Parameter name: decoder_layers.16.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 409370624\n",
      "\n",
      "Parameter name: decoder_layers.16.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 410419200\n",
      "\n",
      "Parameter name: decoder_layers.16.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 411467776\n",
      "\n",
      "Parameter name: decoder_layers.16.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 413564928\n",
      "\n",
      "Parameter name: decoder_layers.16.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 413565952\n",
      "\n",
      "Parameter name: decoder_layers.16.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 419857408\n",
      "\n",
      "Parameter name: decoder_layers.16.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 423003136\n",
      "\n",
      "Parameter name: decoder_layers.16.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 423004160\n",
      "\n",
      "Parameter name: decoder_layers.17.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 425101312\n",
      "\n",
      "Parameter name: decoder_layers.17.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 426149888\n",
      "\n",
      "Parameter name: decoder_layers.17.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 427198464\n",
      "\n",
      "Parameter name: decoder_layers.17.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 429295616\n",
      "\n",
      "Parameter name: decoder_layers.17.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 429296640\n",
      "\n",
      "Parameter name: decoder_layers.17.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 435588096\n",
      "\n",
      "Parameter name: decoder_layers.17.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 438733824\n",
      "\n",
      "Parameter name: decoder_layers.17.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 438734848\n",
      "\n",
      "Parameter name: decoder_layers.18.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 440832000\n",
      "\n",
      "Parameter name: decoder_layers.18.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 441880576\n",
      "\n",
      "Parameter name: decoder_layers.18.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 442929152\n",
      "\n",
      "Parameter name: decoder_layers.18.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 445026304\n",
      "\n",
      "Parameter name: decoder_layers.18.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 445027328\n",
      "\n",
      "Parameter name: decoder_layers.18.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 451318784\n",
      "\n",
      "Parameter name: decoder_layers.18.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 454464512\n",
      "\n",
      "Parameter name: decoder_layers.18.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 454465536\n",
      "\n",
      "Parameter name: decoder_layers.19.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 456562688\n",
      "\n",
      "Parameter name: decoder_layers.19.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 457611264\n",
      "\n",
      "Parameter name: decoder_layers.19.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 458659840\n",
      "\n",
      "Parameter name: decoder_layers.19.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 460756992\n",
      "\n",
      "Parameter name: decoder_layers.19.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 460758016\n",
      "\n",
      "Parameter name: decoder_layers.19.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 467049472\n",
      "\n",
      "Parameter name: decoder_layers.19.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 470195200\n",
      "\n",
      "Parameter name: decoder_layers.19.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 470196224\n",
      "\n",
      "Parameter name: decoder_layers.20.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 472293376\n",
      "\n",
      "Parameter name: decoder_layers.20.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 473341952\n",
      "\n",
      "Parameter name: decoder_layers.20.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 474390528\n",
      "\n",
      "Parameter name: decoder_layers.20.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 476487680\n",
      "\n",
      "Parameter name: decoder_layers.20.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 476488704\n",
      "\n",
      "Parameter name: decoder_layers.20.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 482780160\n",
      "\n",
      "Parameter name: decoder_layers.20.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 485925888\n",
      "\n",
      "Parameter name: decoder_layers.20.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 485926912\n",
      "\n",
      "Parameter name: decoder_layers.21.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 488024064\n",
      "\n",
      "Parameter name: decoder_layers.21.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 489072640\n",
      "\n",
      "Parameter name: decoder_layers.21.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 490121216\n",
      "\n",
      "Parameter name: decoder_layers.21.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 492218368\n",
      "\n",
      "Parameter name: decoder_layers.21.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 492219392\n",
      "\n",
      "Parameter name: decoder_layers.21.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 498510848\n",
      "\n",
      "Parameter name: decoder_layers.21.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 501656576\n",
      "\n",
      "Parameter name: decoder_layers.21.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 501657600\n",
      "\n",
      "Parameter name: decoder_layers.22.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 503754752\n",
      "\n",
      "Parameter name: decoder_layers.22.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 504803328\n",
      "\n",
      "Parameter name: decoder_layers.22.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 505851904\n",
      "\n",
      "Parameter name: decoder_layers.22.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 507949056\n",
      "\n",
      "Parameter name: decoder_layers.22.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 507950080\n",
      "\n",
      "Parameter name: decoder_layers.22.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 514241536\n",
      "\n",
      "Parameter name: decoder_layers.22.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 517387264\n",
      "\n",
      "Parameter name: decoder_layers.22.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 517388288\n",
      "\n",
      "Parameter name: decoder_layers.23.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 519485440\n",
      "\n",
      "Parameter name: decoder_layers.23.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 520534016\n",
      "\n",
      "Parameter name: decoder_layers.23.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 521582592\n",
      "\n",
      "Parameter name: decoder_layers.23.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 523679744\n",
      "\n",
      "Parameter name: decoder_layers.23.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 523680768\n",
      "\n",
      "Parameter name: decoder_layers.23.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 529972224\n",
      "\n",
      "Parameter name: decoder_layers.23.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 533117952\n",
      "\n",
      "Parameter name: decoder_layers.23.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 533118976\n",
      "\n",
      "Parameter name: decoder_layers.24.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 535216128\n",
      "\n",
      "Parameter name: decoder_layers.24.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 536264704\n",
      "\n",
      "Parameter name: decoder_layers.24.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 537313280\n",
      "\n",
      "Parameter name: decoder_layers.24.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 539410432\n",
      "\n",
      "Parameter name: decoder_layers.24.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 539411456\n",
      "\n",
      "Parameter name: decoder_layers.24.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 545702912\n",
      "\n",
      "Parameter name: decoder_layers.24.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 548848640\n",
      "\n",
      "Parameter name: decoder_layers.24.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 548849664\n",
      "\n",
      "Parameter name: decoder_layers.25.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 550946816\n",
      "\n",
      "Parameter name: decoder_layers.25.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 551995392\n",
      "\n",
      "Parameter name: decoder_layers.25.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 553043968\n",
      "\n",
      "Parameter name: decoder_layers.25.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 555141120\n",
      "\n",
      "Parameter name: decoder_layers.25.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 555142144\n",
      "\n",
      "Parameter name: decoder_layers.25.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 561433600\n",
      "\n",
      "Parameter name: decoder_layers.25.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 564579328\n",
      "\n",
      "Parameter name: decoder_layers.25.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 564580352\n",
      "\n",
      "Parameter name: decoder_layers.26.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 566677504\n",
      "\n",
      "Parameter name: decoder_layers.26.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 567726080\n",
      "\n",
      "Parameter name: decoder_layers.26.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 568774656\n",
      "\n",
      "Parameter name: decoder_layers.26.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 570871808\n",
      "\n",
      "Parameter name: decoder_layers.26.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 570872832\n",
      "\n",
      "Parameter name: decoder_layers.26.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 577164288\n",
      "\n",
      "Parameter name: decoder_layers.26.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 580310016\n",
      "\n",
      "Parameter name: decoder_layers.26.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 580311040\n",
      "\n",
      "Parameter name: decoder_layers.27.gq_attn.q_proj.weight\n",
      "Parameter shape: torch.Size([2048, 1024])\n",
      "\n",
      "Total params: 582408192\n",
      "\n",
      "Parameter name: decoder_layers.27.gq_attn.k_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 583456768\n",
      "\n",
      "Parameter name: decoder_layers.27.gq_attn.v_proj.weight\n",
      "Parameter shape: torch.Size([1024, 1024])\n",
      "\n",
      "Total params: 584505344\n",
      "\n",
      "Parameter name: decoder_layers.27.gq_attn.fc.weight\n",
      "Parameter shape: torch.Size([1024, 2048])\n",
      "\n",
      "Total params: 586602496\n",
      "\n",
      "Parameter name: decoder_layers.27.rms_norm_1.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 586603520\n",
      "\n",
      "Parameter name: decoder_layers.27.swiglu.fc.weight\n",
      "Parameter shape: torch.Size([6144, 1024])\n",
      "\n",
      "Total params: 592894976\n",
      "\n",
      "Parameter name: decoder_layers.27.down_proj.weight\n",
      "Parameter shape: torch.Size([1024, 3072])\n",
      "\n",
      "Total params: 596040704\n",
      "\n",
      "Parameter name: decoder_layers.27.rms_norm_2.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 596041728\n",
      "\n",
      "Parameter name: rms_norm.weight\n",
      "Parameter shape: torch.Size([1024])\n",
      "\n",
      "Total params: 596042752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.shape}\\n\")\n",
    "    total += param.numel()\n",
    "    print(f\"Total params: {total}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
